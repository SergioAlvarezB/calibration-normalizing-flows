{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import tempfile\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.special import softmax\n",
    "from torchvision import datasets, transforms\n",
    "from torch.distributions import MultivariateNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1000\n",
    "dim = 784  # 28x28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Distribution:\n",
    "\n",
    "We want to generate samples from the distribution of MNIST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def tensor_round(tensor):\n",
    "    return torch.round(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(tempfile.gettempdir(), train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Lambda(lambda tensor:min_max_normalization(tensor, 0, 1)),\n",
    "                            transforms.Lambda(lambda tensor:tensor_round(tensor))\n",
    "                        ])),\n",
    "        batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base distribution:\n",
    "\n",
    "We use the factorized distribution multivariate normal with diagonal covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_distr = MultivariateNormal(torch.zeros(dim), torch.eye(dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_size=[], activation=F.relu):\n",
    "        super(MLP, self).__init__()\n",
    "        self.activation = activation\n",
    "        units = [dim] + hidden_size + [dim]\n",
    "        self.layers = nn.ModuleList([nn.Linear(units[i], units[i+1])\n",
    "                                     for i in range(len(units)-1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        y = self.layers[-1](x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineConstantLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, scale=True, shift=True):\n",
    "        super(AffineConstantLayer, self).__init__()\n",
    "\n",
    "        self.s = nn.Parameter(torch.randn(1, dim, requires_grad=True)) \\\n",
    "            if scale else None\n",
    "        self.t = nn.Parameter(torch.randn(1, dim, requires_grad=True)) \\\n",
    "            if shift else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = self.s if self.s is not None else x.new_zeros(x.size())\n",
    "        t = self.t if self.t is not None else x.new_zeros(x.size())\n",
    "        z = x * torch.exp(s) + t\n",
    "        log_det = torch.sum(s, dim=1)\n",
    "        return z, log_det\n",
    "\n",
    "    def backward(self, z):\n",
    "        s = self.s if self.s is not None else z.new_zeros(z.size())\n",
    "        t = self.t if self.t is not None else z.new_zeros(z.size())\n",
    "        x = (z - t) * torch.exp(-s)\n",
    "        log_det = torch.sum(-s, dim=1)\n",
    "        return x, log_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NvpCouplingLayer(nn.Module):\n",
    "    def __init__(self, dim, hidden_size=[1000, 1000], random_mask=True):\n",
    "        super(NvpCouplingLayer, self).__init__()\n",
    "        self.s = MLP(dim, hidden_size)\n",
    "        self.t = MLP(dim, hidden_size)\n",
    "        \n",
    "        # Use random partitioning of the data\n",
    "        mask = np.random.randint(2, size=dim)\n",
    "        if random_mask:\n",
    "            while mask.sum() < 1 or mask.sum() == dim:\n",
    "                mask = np.random.randint(2, size=dim)\n",
    "        else:\n",
    "            mask = np.zeros((1, dim))\n",
    "            mask[:, dim//2:] = 1\n",
    "            \n",
    "        self.mask = nn.Parameter(\n",
    "                torch.as_tensor(mask.copy(), dtype=torch.float),\n",
    "                requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_b = self.mask*x\n",
    "        b_1 = 1 - self.mask\n",
    "\n",
    "        s, t = self.s(x_b), self.t(x_b)\n",
    "        y = x_b + b_1 * (x * torch.exp(s) + t)\n",
    "\n",
    "        log_det = torch.sum(b_1*s, dim=1)\n",
    "\n",
    "        return y.flip((1,)), log_det.squeeze()\n",
    "\n",
    "    def backward(self, x):\n",
    "        x_b = self.mask*x\n",
    "        b_1 = 1 - self.mask\n",
    "\n",
    "        s, t = self.s(x_b), self.t(x_b)\n",
    "        y = x_b + b_1*((x - t) * torch.exp(-s))\n",
    "\n",
    "        log_det = torch.sum(b_1*(-s), dim=1)\n",
    "\n",
    "        return y.flip((1,)), log_det.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flow(nn.Module):\n",
    "    def __init__(self, layers, **kwargs):\n",
    "        super(Flow, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.log_det = nn.Parameter(torch.zeros(1), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cum_log_det = self.log_det.new_full((x.shape[0],), fill_value=0)\n",
    "        for layer in self.layers:\n",
    "            x, log_det = layer(x)\n",
    "            cum_log_det += log_det\n",
    "            \n",
    "        return x, cum_log_det\n",
    "            \n",
    "    def backward(self, x):\n",
    "        cum_log_det = self.log_det.new_full((x.shape[0],), fill_value=0)\n",
    "        for layer in self.layers[::-1]:\n",
    "            x, log_det = layer.backward(x)\n",
    "            cum_log_det += log_det\n",
    "\n",
    "        return x, cum_log_det"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values of gamma to use\n",
    "gammas = [\n",
    "#     lambda x: 0.,\n",
    "#     lambda x: 1.,\n",
    "#     lambda x: 0.,\n",
    "    lambda x: 1,\n",
    "]\n",
    "\n",
    "devices = [\n",
    "#     torch.device('cuda:1'),\n",
    "#     torch.device('cuda:1'),\n",
    "#     torch.device('cuda:1'),\n",
    "    torch.device('cuda:0')\n",
    "]\n",
    "\n",
    "flows = [\n",
    "#     Flow(layers=[AffineConstantLayer(dim) for _ in range(20)]),\n",
    "#     Flow(layers=[AffineConstantLayer(dim) for _ in range(20)]),\n",
    "#     Flow(layers=[NvpCouplingLayer(dim, [1000], random_mask=False) for _ in range(20)]),\n",
    "    Flow(layers=[NvpCouplingLayer(dim, [1000, 1000], random_mask=False) for _ in range(15)]),\n",
    "]\n",
    "\n",
    "labels = [\n",
    "#     r'$\\mathrm{AffineFlow}, \\gamma = 0$',\n",
    "#     r'$\\mathrm{AffineFlow}, \\gamma = 1$',\n",
    "#     r'$\\mathrm{NvpFlow}, \\gamma = 0$',\n",
    "    r'$\\mathrm{NvpFlow}, \\gamma = 1$',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Flows:\n",
    "\n",
    "We train Flows using the forward KL method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(gamma, model, dev):\n",
    "    \n",
    "    base_distr = MultivariateNormal(torch.zeros(dim).to(dev, non_blocking=True), torch.eye(dim).to(dev, non_blocking=True))\n",
    "    model = model.to(dev, non_blocking=True)\n",
    "    \n",
    "    # Instantiate optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    \n",
    "    loss = []\n",
    "    logprior = []\n",
    "    logdet = []\n",
    "    \n",
    "    # Train loop\n",
    "    t0 = time.time()\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        cum_loss = torch.zeros(1, device=dev)\n",
    "        cum_logdet = torch.zeros(1, device=dev)\n",
    "        cum_logprior = torch.zeros(1, device=dev)\n",
    "        \n",
    "        _gamma = gamma(e)\n",
    "        for images, _ in train_loader:\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            \n",
    "            images = images.to(dev, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            preds, _logdet = model.backward(images)\n",
    "\n",
    "            _logdet = torch.mean(_logdet)\n",
    "            _logprior = -torch.mean(base_distr.log_prob(preds))\n",
    "            _loss = _logprior - _gamma*_logdet\n",
    "            \n",
    "            \n",
    "            cum_loss += _loss\n",
    "            cum_logprior += _logprior\n",
    "            cum_logdet += _logdet\n",
    "            \n",
    "            _loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        loss.append(cum_loss.item()/len(train_loader.dataset))\n",
    "        logprior.append(cum_logprior.item()/len(train_loader.dataset))\n",
    "        logdet.append(cum_logdet.item()/len(train_loader.dataset))\n",
    "            \n",
    "        if e%10 == 9:\n",
    "            print('epoch: {}, at time: {:.2f}, loss: {:.3f}'.format(e, time.time()-t0, loss[-1]))\n",
    "        \n",
    "        \n",
    "    return {\n",
    "        'model': model.to('cpu'),\n",
    "        'loss': loss,\n",
    "        'logprior': logprior,\n",
    "        'logdet': logdet,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, at time: 576.43, loss: -0.701\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-9f7db4071e89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgammas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-9f7db4071e89>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgammas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-845572b0e584>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(gamma, model, dev)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0m_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcum_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\calibration\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = [train_model(gamma, flow, dev) for gamma, flow, dev in zip(gammas, flows, devices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training NLL\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    label = labels[i]\n",
    "    ax[0].plot(model['loss'], label=label)\n",
    "    ax[1].plot(model['logprior'], label=label)\n",
    "    ax[2].plot(model['logdet'], label=label)\n",
    "\n",
    "\n",
    "ax[0].set_title('Training Loss')\n",
    "ax[0].set_ylabel('logprior + log(Det)')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylim([-3, 0])\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('logprior')\n",
    "ax[1].set_ylabel('logprior')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].set_title('log-det')\n",
    "ax[2].set_ylabel('log(Det)')\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_distr.sample((16,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, _ = models[0]['model'](x)\n",
    "\n",
    "z = z.view(-1, 28, 28).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_arr = plt.subplots(4, 4, figsize=(18, 18))\n",
    "\n",
    "for i, ima in enumerate(z):\n",
    "    ax_arr[i%4, i//4].imshow(z[i, :])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
