{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NICE for calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_pdf_simplex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-16642ef778eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monehot_encode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_temperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_log_likelihood_ratios\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_pdf_simplex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_prob_simplex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreliability_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mECE_plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAddCouplingLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcalibrators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPAVCalibrator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMLRCalibrator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_pdf_simplex'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Activation, Input\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "from utils.ops import onehot_encode, optim_temperature, detection_log_likelihood_ratios\n",
    "from utils.metrics import neg_log_likelihood\n",
    "from utils.visualization import plot_pdf_simplex, plot_prob_simplex, reliability_plot, ECE_plot\n",
    "from flows.nice import AddCouplingLayer, MLP\n",
    "from calibrators import PAVCalibrator, MLRCalibrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define NICE flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "activation='relu'\n",
    "hidden_size = [3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coupling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = MLP(input_dim//2, input_dim - input_dim//2, activation=activation, hidden_size=hidden_size)\n",
    "m2 = MLP(input_dim - input_dim//2, input_dim//2, activation=activation, hidden_size=hidden_size)\n",
    "m3 = MLP(input_dim//2, input_dim - input_dim//2, activation=activation, hidden_size=hidden_size)\n",
    "m4 = MLP(input_dim - input_dim//2, input_dim//2, activation=activation, hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(input_dim,))\n",
    "\n",
    "x = AddCouplingLayer(m1)(inp)\n",
    "x = AddCouplingLayer(m2, mode='even')(x)\n",
    "x = AddCouplingLayer(m3)(x)\n",
    "x = AddCouplingLayer(m4, mode='even')(x)\n",
    "\n",
    "nice_flow = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_inp = Input(shape=(input_dim,))\n",
    "\n",
    "x = AddCouplingLayer(m4, mode='even', inverse=True)(inv_inp)\n",
    "x = AddCouplingLayer(m3, inverse=True)(x)\n",
    "x = AddCouplingLayer(m2, mode='even', inverse=True)(x)\n",
    "x = AddCouplingLayer(m1, inverse=True)(x)\n",
    "\n",
    "inv_nice_flow = Model(inputs=inv_inp, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax output layer\n",
    "y = Activation('softmax')(nice_flow.output)\n",
    "\n",
    "nice_calibrator = Model(inputs=nice_flow.input, outputs=y)\n",
    "nice_calibrator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison against other calibration methods\n",
    "\n",
    "### Generate fake target disitribution and simulated predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array(random.choices(range(3), k=n_samples))\n",
    "one_hot = onehot_encode(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift\n",
    "offset = np.zeros((n_samples, 3))\n",
    "offset[:, 1:] = 0.3\n",
    "\n",
    "# Twist\n",
    "twisting = np.zeros((n_samples, 3))\n",
    "twisting[np.arange(n_samples), target-1] = 0.7 + np.random.randn(n_samples)*0.1\n",
    "\n",
    "snt_logits = 0.8 * (one_hot + twisting + np.random.randn(n_samples, 3)*0.3) + offset\n",
    "snt_probs = softmax(snt_logits, axis=1)\n",
    "\n",
    "nll = neg_log_likelihood(snt_probs, target)\n",
    "\n",
    "print(\"Negative log-likelihood of the classifier predictions: {:.5f}\".format(nll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_arr = plt.subplots(1, 2, figsize=(17, 6))\n",
    "ax_arr[0] = plot_prob_simplex(snt_probs, target=target, ax=ax_arr[0], title='Simulated classifier predictions', fontsize=12);\n",
    "ax_arr[1] = plot_pdf_simplex(snt_probs, ax=ax_arr[1], title='Estimated output classifier PDF', fontsize=12);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply temperature scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp scaling\n",
    "T = optim_temperature(snt_logits, target)\n",
    "temp_probs = softmax(snt_logits/T, axis=1)\n",
    "\n",
    "nll_temp = neg_log_likelihood(temp_probs, target)\n",
    "\n",
    "print(\"Negative log-likelihood after calibration with temp-scaling: {:.3f}\".format(nll_temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrate with PAV extended to multiclass(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pav_cal = PAVCalibrator(snt_logits, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pav_probs = pav_cal.predict(snt_logits)\n",
    "nll_pav = neg_log_likelihood(pav_probs, target)\n",
    "print(\"Negative log-likelihood after calibration with PAV: {:.3f}\".format(nll_pav))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrate with Multiclass Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_cal = MLRCalibrator(snt_logits, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_probs = mlr_cal.predict(snt_logits)\n",
    "nll_mlr = neg_log_likelihood(mlr_probs, target)\n",
    "print(\"Negative log-likelihood after calibration with mlr: {:.3f}\".format(nll_mlr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibrate with NICE flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_calibrator.compile(optimizer=SGD(1e-3), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = nice_calibrator.fit(snt_logits, one_hot, epochs=10000, batch_size=100, verbose=0)\n",
    "\n",
    "# Plot training NLL\n",
    "plt.plot(h.history['loss'])\n",
    "plt.title('NICE-flow NLL')\n",
    "plt.ylabel('NLL')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_probs = nice_calibrator.predict(snt_logits, batch_size=100)\n",
    "\n",
    "nll_nice = neg_log_likelihood(nice_probs, target)\n",
    "\n",
    "print(\"Negative log-likelihood after calibration with NICE: {:.3f}\".format(nll_nice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_arr = plt.subplots(3, 2, figsize=(16, 22), gridspec_kw={'hspace': 0.5})\n",
    "ax_arr[0, 0] = plot_pdf_simplex(snt_probs, ax=ax_arr[0, 0],\n",
    "                             title='Uncalibrated classifier output.\\n NLL: {:.3f}'.format(nll), fontsize=12);\n",
    "ax_arr[0, 1] = plot_pdf_simplex(temp_probs, ax=ax_arr[0, 1],\n",
    "                             title='Calibrated using temperature scaling.\\n NLL: {:.3f}'.format(nll_temp), fontsize=12);\n",
    "ax_arr[1, 0] = plot_pdf_simplex(pav_probs, ax=ax_arr[1, 0],\n",
    "                             title='Calibrated using PAV.\\n NLL: {:.3f}'.format(nll_pav), fontsize=12);\n",
    "ax_arr[1, 1] = plot_pdf_simplex(mlr_probs, ax=ax_arr[1, 1],\n",
    "                             title='Calibrated using MLR.\\n NLL: {:.3f}'.format(nll_mlr), fontsize=12);\n",
    "ax_arr[2, 0] = plot_pdf_simplex(nice_probs, ax=ax_arr[2, 0],\n",
    "                             title='Calibrated using NICE flow.\\n NLL: {:.3f}'.format(nll_nice), fontsize=12);\n",
    "\n",
    "ax_arr[2, 1] = reliability_plot([snt_probs, temp_probs, pav_probs, mlr_probs, nice_probs], target, ax=ax_arr[2, 1], \n",
    "                                labels=['Uncalibrated probabilities', 'Calibrated using Temp-Scaling',\n",
    "                                        'Calibrated using PAV', 'Calibrated using MLR', 'Calibrated using NICE']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECE plots:\n",
    "\n",
    "Define the detection problem, we have the hypothesis: $H_{t} \\equiv P(\\theta=\\theta_0)$ and the hypothesis space is defined over $\\{H_{t}, \\neg H_{t}\\}$, where $\\neg$ is the negation operator. We evaluate for all possible values of $\\theta_0 = \\{0, 1, 2\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRs = detection_log_likelihood_ratios(snt_logits, np.zeros(3) + 1/3.)\n",
    "nice_logits = nice_flow.predict(snt_logits)\n",
    "LRs_cal = detection_log_likelihood_ratios(nice_logits, np.zeros(3) + 1/3.)\n",
    "LRs_cal_MLR = detection_log_likelihood_ratios(np.log(mlr_probs), np.zeros(3) + 1/3.)\n",
    "\n",
    "fig, ax_arr = plt.subplots(3, 3, figsize=(16, 18), gridspec_kw={'hspace': 0.5})\n",
    "\n",
    "\n",
    "# H_t = P(theta=0)\n",
    "pav_cal = IsotonicRegression(y_min=0, y_max=1).fit_transform(snt_probs[:, 0], target==0) \n",
    "pav_cal = pav_cal/(1. - pav_cal + 1e-7) + 1e-7\n",
    "\n",
    "ax_arr[0, 0] = ECE_plot(np.exp(LRs[:, 0]), target==0, cal_ratios=np.exp(LRs_cal[:, 0]), ax=ax_arr[0, 0], title='Calibrated with NICE\\n$P(\\\\theta=0)$')\n",
    "ax_arr[0, 1] = ECE_plot(np.exp(LRs[:, 0]), target==0, cal_ratios=np.exp(LRs_cal_MLR[:, 0]), ax=ax_arr[0, 1], title='Calibrated with MLR\\n$P(\\\\theta=0)$')\n",
    "ax_arr[0, 2] = ECE_plot(np.exp(LRs[:, 0]), target==0, cal_ratios=pav_cal, ax=ax_arr[0, 2], title='Calibrated binary with PAV\\n$P(\\\\theta=0)$')\n",
    "\n",
    "# H_t = P(theta=1)\n",
    "pav_cal = IsotonicRegression(y_min=0, y_max=1).fit_transform(snt_probs[:, 1], target==1) \n",
    "pav_cal = pav_cal/(1. - pav_cal + 1e-7) + 1e-7\n",
    "\n",
    "ax_arr[1, 0] = ECE_plot(np.exp(LRs[:, 1]), target==1, cal_ratios=np.exp(LRs_cal[:, 1]), ax=ax_arr[1, 0], title='Calibrated with NICE\\n$P(\\\\theta=1)$')\n",
    "ax_arr[1, 1] = ECE_plot(np.exp(LRs[:, 1]), target==1, cal_ratios=np.exp(LRs_cal_MLR[:, 1]), ax=ax_arr[1, 1], title='Calibrated with MLR\\n$P(\\\\theta=1)$')\n",
    "ax_arr[1, 2] = ECE_plot(np.exp(LRs[:, 1]), target==1, cal_ratios=pav_cal, ax=ax_arr[1, 2], title='Calibrated binary with PAV\\n$P(\\\\theta=1)$')\n",
    "\n",
    "# H_t = P(theta=2)\n",
    "pav_cal = IsotonicRegression(y_min=0, y_max=1).fit_transform(snt_probs[:, 2], target==2) \n",
    "pav_cal = pav_cal/(1. - pav_cal + 1e-7) + 1e-7\n",
    "\n",
    "ax_arr[2, 0] = ECE_plot(np.exp(LRs[:, 2]), target==2, cal_ratios=np.exp(LRs_cal[:, 2]), ax=ax_arr[2, 0], title='Calibrated with NICE\\n$P(\\\\theta=2)$')\n",
    "ax_arr[2, 1] = ECE_plot(np.exp(LRs[:, 2]), target==2, cal_ratios=np.exp(LRs_cal_MLR[:, 2]), ax=ax_arr[2, 1], title='Calibrated with MLR\\n$P(\\\\theta=2)$')\n",
    "ax_arr[2, 2] = ECE_plot(np.exp(LRs[:, 2]), target==2, cal_ratios=pav_cal, ax=ax_arr[2, 2], title='Calibrated binary with PAV\\n$P(\\\\theta=2)$')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that NICE flow is an invertible transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward flow\n",
    "nice_logits = nice_flow.predict(snt_logits, batch_size=100)\n",
    "# Inverse flow\n",
    "recons_logits = inv_nice_flow.predict(nice_logits, batch_size=100)\n",
    "\n",
    "recons_rmse = np.sqrt(np.mean(np.square(snt_logits-recons_logits)))\n",
    "print(\"Root mean square reconstrucction error is: {:.3E}\".format(recons_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_arr = plt.subplots(2, 3, sharex='col', figsize=(20, 10), gridspec_kw={'hspace': 0, 'wspace': 0.3})\n",
    "fig.suptitle('The trained NICE-flow is an invertible transformation', fontsize=20)\n",
    "\n",
    "ax_arr[0, 0].scatter(snt_logits[:, 0], snt_logits[:, 1]);\n",
    "ax_arr[1, 0].scatter(snt_logits[:, 0], snt_logits[:, 2]);\n",
    "\n",
    "ax_arr[0, 1].scatter(nice_logits[:, 0], nice_logits[:, 1]);\n",
    "ax_arr[1, 1].scatter(nice_logits[:, 0], nice_logits[:, 2]);\n",
    "\n",
    "ax_arr[0, 2].scatter(recons_logits[:, 0], recons_logits[:, 1]);\n",
    "ax_arr[1, 2].scatter(recons_logits[:, 0], recons_logits[:, 2]);\n",
    "\n",
    "titles = [\n",
    "    'Classifier output logits', \n",
    "    'Transformed logits', \n",
    "    'Reconstructed logits\\n Reconstruccion error: {:.3E}'.format(recons_rmse)\n",
    "]\n",
    "y_labels = ['Z($\\\\theta$=1)', 'Z($\\\\theta$=2)']\n",
    "\n",
    "for ax, title in zip(ax_arr[0], titles):\n",
    "    ax.set_title(title, fontsize=16)\n",
    "\n",
    "for ax in ax_arr[-1]:\n",
    "    ax.set_xlabel('Z($\\\\theta$=0)', fontsize=14)\n",
    "\n",
    "for ax, label in zip(ax_arr[:,0], y_labels):\n",
    "    ax.set_ylabel(label, fontsize=14)\n",
    "\n",
    "# fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
